\section{Representing and Ontological Realms}
Two types of ontologies are used by our system:
\begin{enumerate}
\item Realm ontologies, comprising a heterarchy of categories that
  characterize terms appearing in queries.
\item SSQ ontologies, describing the form of particular
  realm-specific queries.
\end{enumerate}
For any particular ontological realm, the number of potential
terms and their associated categories can be quite large.
Manual construction of a realm ontology and the terms associated
with its categories would be an extremely laborious task
for any non-trivial realm. For example, the realm of vehicles
in our current ontology contains 32 categories and millions of terms.
We were unable to find
an existing ontology for this realm, so we constructed one
using categories from the DBPedia \cite{Auer07dbpedia:a}.
The primary benefit of using the DBPedia for this purpose is
that not only does it supply a categorical heterarchy, but it
also links to Wikipedia documents which provide us with a large
realm-specific corpus.

\subsection{Category Heterarchy} 

To support our process, we map DBPedia categories into classes in our
realm ontology with the \textit{superclass} and \textit{subclass}
relationships built using the DBPedia properties \textit{broader} and
\textit{narrower}. This semantic heterarchy can be easily represented
using the OWL ontology and by using OWL, we can exploit the OWL API
for matching SSQ ontologies that reference these realm ontologies.


%Start with a collection of relevant categories 

Once the semantic heterarchy is created, the next step is to create
training data for the system. To do this, we extract words from the pages,
thus building a corpus related to a given category.
Finally, we have compiled enough information to properly characterize
a category while limiting the
amount of unnecessary content within the corpus. The problem with this
approach is the page density varies greatly across the
heterarchy. Each category within the DBpedia and Wikipedia heterarchy
contains a set of information pages, P, corresponding to the concepts
shared by the category. Since the categorical structure is a directed
acyclic graph \cite{Suchanek07yago:a}, we use of a Markov Blanket
\cite{Friedman97bayesiannetwork} to describe a particular
category. We hypothesize that the terms associated with a category
can be found by identifying those terms associated with that category's
ancestor, descendant, and spousal nodes (i.e., other parent nodes of a subcategory). While this does not completely solve the
issue of a category residing in a sparse area of the graph, the amount
of information that can be obtained per blanket is more
helpful than per category.

Clustering the categories alone does not guarantee adequate corpus
creation. Since our model relies on the subcategories to
to provide terms associated with a given category, it will fail if
the subcategories do not provide enough data to our corpus.
For the purpose of gathering training data having more subcategory data
yields a more representative collection of terms for that category. 
The average number of pages per category in the Wikipedia is 25.7\cite{1321474}.
We have found that this number of pages is insufficient to provide a representative collection of terms for a category.
In order to gain more pages and expand the term base, we have used a
PlusOne mechanism to extract
extra information. This mechanism checks if a subcategory has an
adequate number of pages to fully describe its subtopic.
We have chosen this number to be approximately ten times the average
number of Wikipedia pages per category, or 260.  If a cateogory
does not contain the required number of pages, PlusOne continues to
search another level deeper in the hierarchy until a sufficient
number of pages has been reached.


\subsection{Associating Terms with Categories}
\label{sec:terms}
%joir-dan

Our approach to category corpus construction does not take into
account the number of words contained within each page.
However, we have found this method to be adequate to construct
usable corpora. For example, when executed on the vehicle ontology,
this method yields 2255 subject pages and a corpus containing
1,691,727 unique terms and 6,084,260 total words.

After a category's corpus is built, we then build term frequency
distributions and store them in a database for natural language processing.
For this system, term importance refers to the prior probability
of a category given a term. We calculate this using Bayes Rule,
since we can easily obtain the term-category and term-corpus
probabilities. The higher the probability, the more likely a term is
referring to a category.

\begin{equation}
P (category | term) = \frac{P(term | category) P(category)}{P(term)}
\end{equation}
